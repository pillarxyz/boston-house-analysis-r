---
title: "Regression"
output:
  word_document: default
  pdf_document: default
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

## Introduction

Simple linear regression is a statistical technique that uses single explanatory variable to predict the outcome of a response variable when there is more than one explanatory variable we use MLR (Multiple linear regression)

Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable.

Multiple linear regression analysis makes several key assumptions:

* Linear relationship : There exists a linear relationship between each predictor variable and the response variable.
* Multivariate normality : The residuals of the model are normally distributed.
* No or little multi collinearity :  None of the predictor variables are highly correlated with each other.
* No auto-correlation : There is no autocorrelation between errors.
* Homoscedasticity : The residuals have constant variance at every point in the linear model.


```{r}
library(readxl)
library(randomForest)
library(caret)
```

```{r}
data = read_excel("housing.xls")
head(data)
```

## Data Preperation

After visualizing our target variable we see that it's a little bit skewed to the right and not normally distributed

```{r}
densityplot(data$MEDV)
```


we partition our data into train and test datasets so we can benchmark the performance of our model

```{r}
set.seed(12345)

inTrain <- createDataPartition(y = data$MEDV, p = 0.70, list = FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]
```

## Linear Regression

We use a simple linear regression model at first as a baseline model to compare our improvements to.

```{r}
fit.lm <- lm(MEDV~.,data = training)
summary(fit.lm)
```
The above output shows that :

* Most coefficients are statistically significant since p-value < 0.05
* The model is also statistically significant since p-value: 2.2e-16 < 0.05
* Adjusted R-squared: 0.7014 , This shows that 70.14% of the variation available in the given dataset (in MEDV) is explained by this Linear Regression Model. Rest the variation in MEDV is due to some other predictors or due to random cause.
* the star symbols signify the relevance of each feature, more stars indicate stronger correlation to the dependant variable while less indicate weaker correlation.

already decent model but we can do better


```{r}
set.seed(12345)

pred.lm <- predict(fit.lm, newdata = testing)


rmse.lm <- sqrt(sum((pred.lm - testing$MEDV)^2)/
                   length(testing$MEDV))
c(RMSE = rmse.lm, R2 = summary(fit.lm)$r.squared)
```


Our baseline RMSE (root mean squared error) is 3.919 which we will aim to improve on using feature selection and transformations

```{r}
plot(pred.lm,testing$MEDV, xlab = "Predicted Price", ylab = "Actual Price", col="blue")
```


The plots below are representations of the model's residuals (or errors) the more they look like random noise the better as your independent variables should describe the relationship so thoroughly that only random error remains.
```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(fit.lm)
```


We try a log transform on our target variable as to make it more normally distributed the thing that really did improve our RMSE by a significant amount.

```{r}
set.seed(12345)
#Try linear model using all features
fit.lm1 <- lm(log(MEDV)~.,data = training)

set.seed(12345)
#predict on test set
pred.lm1 <- predict(fit.lm1, newdata = testing)

# Root-mean squared error
rmse.lm1 <- sqrt(sum((exp(pred.lm1) - testing$MEDV)^2)/
                   length(testing$MEDV))
                   
c(RMSE = rmse.lm1, R2 = summary(fit.lm1)$r.squared)
```
```{r}
summary(fit.lm1)
```
```{r}
plot(exp(pred.lm1),testing$MEDV, xlab = "Predicted Price", ylab = "Actual Price", col="blue")
```
```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(fit.lm1)
```

```{r}
set.seed(12345)
#Try simple linear model using selected features
fit.lm2 <- lm(formula = log(MEDV) ~ CRIM + CR01 + CHAS + NOX + RM + DIS + 
            RAD + TAX + PTRATIO + LSTAT, data = training)

set.seed(12345)
#predict on test set
pred.lm2 <- predict(fit.lm2, newdata = testing)

# Root-mean squared error
rmse.lm2 <- sqrt(sum((exp(pred.lm2) - testing$MEDV)^2)/
                   length(testing$MEDV))
                   
c(RMSE = rmse.lm2, R2 = summary(fit.lm2)$r.squared)
```


## Non-Linear Regression

Linear regression models are kinda limited due to them being linear and especially when the data we have is a bit noisy and has some of outliers so we test a random forest model which based on decision trees for our regression, to see if a non-linear regression model can do better


```{r}
set.seed(12345)
fit.rf <- randomForest(formula = MEDV ~ CRIM + CHAS + NOX + RM + DIS + RAD + TAX + PTRATIO + LSTAT, data = training)

set.seed(12345)
pred.rf <- predict(fit.rf, newdata = testing)

# Root-mean squared error
rmse.rf <- sqrt(sum((pred.rf - testing$MEDV)^2)/
                   length(testing$MEDV))
c(RMSE = rmse.rf, R2 = 1 - (sum((testing$MEDV-pred.rf)^2)/sum((testing$MEDV-mean(testing$MEDV))^2)))
```

We see a large improvement of RMSE after using the random forest model confirming that our linear regression models are relatively restricted in how they can fit data due to them being limited to capturing linear relationships and being sensitive to outliers unlike non-linear regression models, however non-linear models can sometimes overfit and they are also relatively harder to interpret compared linear models.
```{r}
plot(fit.rf)
```

```{r}
plot(pred.rf,testing$MEDV, xlab = "Predicted Price", ylab = "Actual Price", col="blue")
```
